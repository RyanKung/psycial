# Psycial - MBTI Classifier Configuration

[data]
# Path to training data
csv_path = "data/mbti_1.csv"
# Train/test split ratio (0.8 = 80% train, 20% test)
train_split = 0.8

[features]
# Maximum number of TF-IDF features
max_tfidf_features = 5000
# BERT model automatically provides 384 dimensions

# Psychological features from SEANCE, TAACO, TAALES frameworks
# Set to true to integrate psychological features (emotion, cohesion, lexical sophistication)
use_psychological_features = false
# Feature type: "simple" (9 features), "selected" (108 features), "full" (930 features)
# - simple: Top 9 emotion/power features (fast, ~0.1s overhead)
# - selected: 108 features via Pearson selection (recommended, ~30s overhead)
# - full: All 930 features (slow, may need more epochs)
psy_feature_type = "selected"

[model]
# Model type: "single" (16-way classification) or "multitask" (4 x 2-way classification)
# Multitask is more aligned with MBTI theory and often achieves better results
# Can be overridden via command-line: --multi-task or --single-task
model_type = "multitask"

# Network architecture (hidden layer sizes)
# Paper uses 8-layer Transformer with 108 features -> we use simpler but effective architecture
hidden_layers = [768, 512, 256]
# Learning rate for Adam optimizer
learning_rate = 0.001
# Dropout rate for regularization (0.0-1.0)
# Higher values = stronger regularization, less overfitting
# Paper: implicit in Transformer; we use 0.6 as balanced value
dropout_rate = 0.6
# Weight decay (L2 regularization) for Adam optimizer (0.0-0.1)
# Typical values: 0.0001-0.01. Higher values = stronger regularization
weight_decay = 0.01

[training]
# Number of training epochs (for single-task model)
# Paper uses 50+50 (two-stage); we use 20 for single-stage
epochs = 20
# Batch size for training
batch_size = 64
# BERT batch processing size
bert_batch_size = 64

# Per-dimension epochs for multi-task model [E/I, S/N, T/F, J/P]
# Adjusted based on data distribution imbalance:
# - E/I: 25 epochs (77% vs 23%, moderate)
# - S/N: 35 epochs (86% vs 14%, MOST imbalanced) ← needs more training
# - T/F: 20 epochs (54% vs 46%, MOST balanced) ← needs less training
# - J/P: 25 epochs (60% vs 40%, moderate)
per_dimension_epochs = [25, 35, 20, 25]

[output]
# Directory to save trained models
model_dir = "models"
# Model file names (base names, suffix will be added automatically)
# Multi-task models: *_multitask.json, *_multitask.pt
# Single-task models: *_single.json, *_single.pt, class_mapping_single.json
tfidf_file = "tfidf_vectorizer.json"
mlp_file = "mlp_weights.pt"
class_mapping_file = "class_mapping.json"

# Experiment notes:
# - dropout_rate 0.5 with epochs 25: Good balance, test ~49%
# - dropout_rate 0.6 with epochs 20: Stronger regularization
# - dropout_rate 0.4 with epochs 40: Overfitting (train 99%, test 49%)
#
# With psychological features:
# - use_psychological_features = true, psy_feature_type = "selected"
#   Expected: 5000 (TF-IDF) + 384 (BERT) + 108 (Psy) = 5492 features
#   Target accuracy: 60-75% (vs 55-60% without psy features)
#   Training time: +30s for feature extraction and selection

